---
layout: post
title: Blog Post 2: Spectral Clustering
---

# Blog Post 2: Spectral Clustering

In this blog post, we'll create a simple version of the *spectral clustering* algorithm for clustering data points. 

First, let us understand why spectral clustering is an important tool. It's very useful in identifying data that has a complex structure. Let's begin with an example where we wouldn't need clustering. 


```python
#importing the libraries
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x11f199190>

![output_2_1.png](/images/output_2_1.png)

    
Here, we try to cluster (ie. identify the data as two separate blobs). One common way of doing this is through K-means clustereing. 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x11fd1b4c0>


![output_4_1.png](/images/output_4_1.png)

    
K-means worked well here, but now let's consider data that isn't shaped so conveniently...

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x10b8680d0>



![output_6_1.png](/images/output_6_1.png)

    
We can see that there still are clusters in the data, but now they aren't blobs but crescents. K-means won't work here because it looks for circular clusters by design. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
km.predict(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x11feadfa0>



![output_8_1.png](/images/output_8_1.png)


    


Whoops! That's not right! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. 


First, we begin by creating a similariy matrixx of shape (n,n) that contains the distances between each pair of coordinates in the matrix. The library `sklearn` has a function we can use!


```python
from sklearn.metrics.pairwise import pairwise_distances
#Computes all the pairwise distances and collects them into a matrix
A=pairwise_distances(X)
A
```


    array([[0.        , 1.27292462, 1.33315598, ..., 1.9812102 , 1.68337039,
            1.94073324],
           [1.27292462, 0.        , 1.46325112, ..., 1.93729167, 1.68543003,
            1.91287315],
           [1.33315598, 1.46325112, 0.        , ..., 0.64857172, 0.35035968,
            0.60860868],
           ...,
           [1.9812102 , 1.93729167, 0.64857172, ..., 0.        , 0.30070415,
            0.04219636],
           [1.68337039, 1.68543003, 0.35035968, ..., 0.30070415, 0.        ,
            0.26255757],
           [1.94073324, 1.91287315, 0.60860868, ..., 0.04219636, 0.26255757,
            0.        ]])



Now, we want to code these distances into two categories: 1) if the distance is within a specified epsilon, the entry will be 1. If it's not within this distance, the entry will be 0. We need to make sure that the diagonal should be all zeroes because the distance between an entry and itself is 0.

In this part, we use epsilon as 0.4.


```python
#Uses boolean indexing to replace values of the matrixx
A[A>0.4]=0
A[A!=0]=1
#fills the diagonal with zeros
np.fill_diagonal(A,0)
```


```python
#checks that our matrix is symmetric
np.all(A.T==A)
```

    True

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`. 

To do this, we'll computer the binary norm cut objective for the matrix. However, for that, we first need to calculate the cut term and the volume for the a cluster. We'll find that a pair of clusters $$ C_0 $$ and $$ C_1 $$ is considered to be a "good" partition of the data when their norm-cut objective is small. 

First, we compute the cut term, which is the number of nonzero entries in a matrix that relates points in cluster $$C_0$$ to points in cluster $$C_1$$. We don't want the the points in $$C_0$$ to be very close to points in $$C_1$$. 

We write a function `cut(A,y)` to compute the cut term by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 


```python
def cut(A,y):
    #this list contains the elements in cluster 0
    c0 = [i for i in range(200) if y[i]==0]
    #this list contains the elements in cluster 1
    c1 = [i for i in range(200) if y[i]==1]
    #This is where the sum of all entries will be stored
    sum = 0
    #A nested for loop that iterates through each pair of i,j in matrix A
    for i in c0:
        for j in c1:
            #Adds the value of the i,j entry to the sum of all entries 
            sum += A[i,j]
    return sum
```


```python
cut1 = cut(A,y)
```


```python
cut1
```




    13.0

For our true clusters `y`, we get the cut objective as $$13$$. Now, we generate a random vector of random labels, and test our cut function on that.



```python
v = np.random.randint(0, 2, size = n)
v
cut2= cut(A,v)
cut2
```




    1116.0


We find that the cut term for the true clusters is much lower! This shows that this part of the cut objective indeed favors the true clusters over the random ones. 

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. If we choose cluster $$C_0$$ to be small, then $$\mathbf{vol}(C_0)$$ will be small and $$\frac{1}{\mathbf{vol}(C_0)}$$ will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

Write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$, returning them as a tuple. For example, `v0, v1 = vols(A,y)` should result in `v0` holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. Then, write a function called `normcut(A,y)` which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 

***Note***: No for-loops in this part. Each of these functions should be implemented in five lines or less. 


```python
def vols(A,y):
    #Finds the ith row sum of A
    d = np.sum(A,axis=0)
    #Calculates this sum for cluster C0
    v0 = d[y==0].sum()
    #Calculates this sum for cluster C1
    v1 = d[y==1].sum()
    return v0,v1
def normcut(A,y):
    #Gets the volume term for C0
    v0 = vols(A,y)[0]
    #Gets the volume term for C1
    v1 = vols(A,y)[1]
    #Cut term
    cut_var = cut(A,y)
    return cut_var*((1/v0)+(1/v1))
```

Now, compare the `normcut` objective using both the true labels `y` and the fake labels you generated above. What do you observe about the normcut for the true labels when compared to the normcut for the fake labels? 


```python
norm = normcut(A,y)
norm
```




    0.011518412331615225




```python
norm_fake= normcut(A,v)
norm_fake
```




    0.995836336735081



## Part C

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $$A$$ and (b) not too small. One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets. We need a math trick! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$$$


Note that the signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$: if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

Next, if you like linear algebra, you can show that 

$$$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

1. Write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`, using the formula above. 
2. Then, check the equation above that relates the matrix product to the normcut objective, by computing each side separately and checking that they are equal. 
3. While you're here, also check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$, where $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 

#### Programming Note

You can compute $$\mathbf{z}^T\mathbf{D}\mathbf{z}$$ as `z@D@z`, provided that you have constructed these objects correctly. 

#### Note

The equation above is exact, but computer arithmetic is not! `np.isclose(a,b)` is a good way to check if `a` is "close" to `b`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify. 

Also, still no for-loops. 


```python
def transform(A,y):
    #Gets the volume term for C0
    v0 = vols(A,y)[0]
    #Gets the volume term for C1
    v1= vols(A,y)[1]
    #Creates array of length n
    z=np.zeros(n)
    #Sets the value of z[i] to 1/v0 if y[i]==0
    z[y==0]=1/v0
    #Sets the value of z[i] to -1/v1 if y[i]==1
    z[y==1]=-1/v1
    return z
z = transform(A,y)
D = np.zeros((n,n))
d = np.sum(A,axis=0)
np.fill_diagonal(D, d)
z.T@((D-A)@z)/(z@D@z)
```




    0.011518412331615133




```python
z@D@np.ones(n)
```




    0.0



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$$$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this for you. 

Use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_`. 


```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
from scipy.optimize import minimize
#Minimizes the function orth_obj
z_ = minimize(orth_obj,z,method = 'Nelder-Mead')
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. 

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

Does it look like we came close to correctly clustering the data? 


```python
z_["x"]
```




    array([-4.20437555e-04, -4.57677660e-04,  4.36966158e-04,  4.32467189e-04,
            4.35816253e-04,  4.39318864e-04,  4.23546595e-04,  4.52575745e-04,
           -4.57547341e-04, -4.54049848e-04, -4.65803807e-04,  4.40990317e-04,
           -8.41762360e-05, -4.58080413e-04, -4.55507477e-04, -4.56357583e-04,
           -4.57543601e-04,  4.40777120e-04,  4.42268525e-04,  4.61143979e-04,
           -4.56024960e-04, -4.11991173e-04, -4.60153817e-04,  4.32434780e-04,
            4.53772912e-04, -4.65319572e-04,  4.13555222e-04, -4.60579253e-04,
           -4.58511429e-04,  4.29425972e-04,  4.07489688e-04, -4.32782817e-04,
           -4.57459603e-04, -4.58962970e-04, -4.58408721e-04, -4.55544898e-04,
            4.24588715e-04, -4.57755864e-04, -4.56138917e-04,  4.40837174e-04,
           -4.57159927e-04,  4.40516420e-04,  4.27345879e-04,  4.12774664e-04,
            4.45571949e-04,  4.40314649e-04,  4.39634032e-04, -4.57351661e-04,
           -4.53918909e-04, -3.32937768e-04,  4.31193883e-04,  4.30679328e-04,
           -4.60063362e-04, -4.58101595e-04,  4.65417903e-04,  4.38762350e-04,
           -4.56070267e-04, -4.53511445e-04, -4.60343564e-04,  4.42781186e-04,
            4.54329598e-04,  4.35743396e-04, -4.56047930e-04,  4.41520101e-04,
           -4.56668120e-04,  4.33494505e-04,  4.49436746e-04,  4.65829798e-04,
            4.39967155e-04, -4.61870700e-04, -4.58890729e-04, -4.55780906e-04,
           -4.34319873e-04,  4.42428794e-04,  4.39274635e-04,  4.13477301e-04,
           -4.44857205e-04,  4.40818003e-04, -4.58116435e-04,  4.62256464e-04,
            4.15294351e-04,  4.36345166e-04,  4.41940311e-04, -4.55428233e-04,
           -4.47336805e-04, -4.56979109e-04, -4.58970133e-04,  4.36893265e-04,
            4.34640218e-04,  4.41103856e-04,  3.96173375e-04, -4.58214512e-04,
            4.39810085e-04,  4.12971321e-04, -4.03581562e-04, -4.51670983e-04,
           -4.56146287e-04, -4.55735509e-04,  4.38246302e-04,  4.29433585e-04,
           -4.58670004e-04, -4.55228759e-04, -4.55924812e-04,  3.41970264e-04,
           -4.56957777e-04, -4.57163189e-04,  4.49278901e-04,  4.16456485e-04,
           -4.57560920e-04, -4.56495210e-04,  4.55991268e-04,  4.41728595e-04,
            4.37783082e-04, -4.54236461e-04,  4.37416411e-04,  4.34070337e-04,
            4.59128162e-04,  3.89262557e-04, -4.87275286e-04, -4.58048927e-04,
           -4.59690040e-04,  4.48042902e-04, -4.55362191e-04, -4.50124502e-04,
           -4.47657800e-04,  4.40836858e-04, -4.57558723e-04,  4.17967435e-04,
           -4.54481825e-04, -4.16382922e-04,  4.35342488e-04,  4.39068516e-04,
            4.42129449e-04,  4.12235969e-04, -4.56942299e-04, -4.58399416e-04,
           -4.45470580e-04, -4.59908278e-04, -4.55205096e-04, -4.91392713e-04,
           -4.57998907e-04, -4.55874950e-04,  4.41873415e-04,  4.38466116e-04,
           -4.73514825e-04,  4.41922732e-04,  4.34019293e-04,  4.43164490e-04,
            4.37856119e-04,  4.36307583e-04,  4.35527350e-04,  4.39871835e-04,
            4.37512538e-04, -4.59656968e-04, -4.55260142e-04,  4.33471468e-04,
           -4.52854303e-04,  4.33309256e-04,  4.41598988e-04,  4.41186529e-04,
           -4.54859251e-04, -4.73801752e-04,  4.33463498e-04,  4.40070343e-04,
           -4.40920179e-04, -4.59534145e-04, -4.56261231e-04,  4.38566178e-04,
            4.32188882e-04,  4.30501534e-04, -4.56007026e-04,  4.40780637e-04,
           -4.56615339e-04, -4.56700632e-04, -4.56981661e-04,  4.40174644e-04,
            4.13058155e-04,  4.42479540e-04,  4.32386123e-04, -4.44426430e-04,
           -4.58939936e-04,  4.37350709e-04,  4.42029490e-04, -4.43975156e-04,
           -4.58908150e-04, -4.59109858e-04,  4.45791394e-04, -4.57952232e-04,
           -4.55365471e-04, -4.59160496e-04,  4.42380400e-04, -4.55144888e-04,
            4.26685153e-04, -4.56850849e-04, -4.56029674e-04, -4.55832100e-04,
           -4.54472020e-04,  4.40654264e-04,  4.31345925e-04,  4.37347634e-04])




```python
z_min=z_["x"]
#Encodes the z_min matrix to have 0 or 1 according to its sign 
z_min[z_min>=0]=1
z_min[z_min<0]=0
z_min
```




    array([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
           1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
           0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
           1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
           1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
           0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
           0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
           0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
           0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
           0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
           0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
           0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.])




```python
plt.scatter(X[:,0], X[:,1],c=z_min)
```




    <matplotlib.collections.PathCollection at 0x1210e6370>


![output_35_1.png](/images/output_35_1.png)

    
    


## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$$$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $$\mathbf{z}$$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$$$

which is equivalent to the standard eigenvalue problem 

$$$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Construct the matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$, which is often called the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. Then, plot the data again, using the sign of `z_eig` as the color. How did we do? 


```python
D_inv = np.linalg.inv(D) #Inverse of the D matrix
L = D_inv@(D-A) #Creates the matrix L
Lam, U = np.linalg.eig(L) #Finds the eigenvalues and their corresponding eigenvectors
```


```python
z_eig=U[:,1] #Finds the eigenvector corresponding to the second smallest eigenvalue
```


```python
#Encodes the z_eig matrix to have 0 or 1 according to its sign 
z_eig[z_eig>=0]=1
z_eig[z_eig<0]=0
z_eig
```




    array([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
           0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
           1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
           0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
           0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,
           1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
           1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
           1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
           1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
           1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
           1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
           1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.])




```python
plt.scatter(X[:,0], X[:,1],c=z_eig)
```




    <matplotlib.collections.PathCollection at 0x1212c4130>



    
![output_40_1.png](/images/output_40_1.png)



In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

Synthesize your results from the previous parts. In particular, write a function called `spectral_clustering(X, epsilon)` which takes in the input data `X` (in the same format as Part A) and the distance threshold `epsilon` and performs spectral clustering, returning an array of binary labels indicating whether data point `i` is in group `0` or group `1`. Demonstrate your function using the supplied data from the beginning of the problem. 

#### Notes

Despite the fact that this has been a long journey, the final function should be quite short. You should definitely aim to keep your solution under 10, very compact lines. 

**In this part only, please supply an informative docstring!** 

#### Outline

Given data, you need to: 

1. Construct the similarity matrix. 
2. Construct the Laplacian matrix. 
3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix. 
4. Return labels based on this eigenvector. 


```python
def spectral_clustering(X, epsilon):
    '''
    Takes it data and a distance threshold and performs spectral 
    clustering to return an array of binary labels indicating whether 
    data point i is in group 0 or group 1.  
    
    Parameters: 
        X (array): The data to be clustered
        epsilon (int or float): An int/float specifying the distance threshold
        
    Returns: 
        z_eig (array): An array of binary labels indicating whether 
                       data point i is in group 0 or group 1
    '''
    #Computes all the pairwise distances and collects them into a matrix
    A=pairwise_distances(X)
    #Replaces the values according to the distance threshold
    A[A>epsilon]=0
    A[A!=0]=1
    #Fills the diagonal of the matrix with 0
    np.fill_diagonal(A,0)
    #Creates a diagonal matrix containing the sum of the ith row of A
    D = np.diag(A.sum(axis = 1))
    #Inverse of matrix D 
    D_inv = np.linalg.inv(D)
    L = D_inv@(D-A)
    #Finds the eigenvalues and their corresponding eigenvectors
    Lam, U = np.linalg.eig(L)
    #Finds the eigenvector corresponding to the second smallest eigenvalue
    z_eig=U[:,1]
    #Encodes the z_eig matrix to have 0 or 1 according to its sign 
    z_eig[z_eig>=0]=1
    z_eig[z_eig<0]=0
    return z_eig
```

## Part H

Run a few experiments using your function, by generating different data sets using `make_moons`. What happens when you increase the `noise`? Does spectral clustering still find the two half-moon clusters? For these experiments, you may find it useful to increase `n` to `1000` or so -- we can do this now, because of our fast algorithm! 


```python
np.random.seed(2000)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x12270f400>


![output_45_1.png](/images/output_45_1.png)


    



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.7))
```




    <matplotlib.collections.PathCollection at 0x120bdfdf0>

![output_46_1.png](/images/output_46_1.png)


    
    


## Part I

Now try your spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x120cab7f0>


![output_48_1.png](/images/output_48_1.png)

    


There are two concentric circles. As before k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x12649adf0>


![output_50_1.png](/images/output_50_1.png)

        


Can your function successfully separate the two circles? Some experimentation here with the value of `epsilon` is likely to be required. Try values of `epsilon` between `0` and `1.0` and describe your findings. For roughly what values of `epsilon` are you able to correctly separate the two rings? 


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```




    <matplotlib.collections.PathCollection at 0x1236c4790>

![output_52_1.png](/images/output_52_1.png)

    


## Part J

Great work! Turn this notebook into a blog post with plenty of helpful explanation for your reader. 



