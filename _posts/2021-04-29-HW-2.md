---
layout: post
title: Blog Post 2: Spectral Clustering
---

# Blog Post 2: Spectral Clustering

In this blog post, we'll create a simple version of the *spectral clustering* algorithm for clustering data points. 

First, let us understand why spectral clustering is an important tool. It's very useful in identifying data that has a complex structure. Let's begin with an example where we wouldn't need clustering. 


```python
#importing the libraries
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```

```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x11f199190>

![output_2_1.png](/images/output_2_1.png)

    
Here, we try to cluster (ie. identify the data as two separate blobs). One common way of doing this is through K-means clustereing. 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x11fd1b4c0>


![output_4_1.png](/images/output_4_1.png)

    
K-means worked well here, but now let's consider data that isn't shaped so conveniently...

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x10b8680d0>



![output_6_1.png](/images/output_6_1.png)

    
We can see that there still are clusters in the data, but now they aren't blobs but crescents. K-means won't work here because it looks for circular clusters by design. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
km.predict(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x11feadfa0>



![output_8_1.png](/images/output_8_1.png)


    


Whoops! That's not right! 

As we'll see, spectral clustering is able to correctly cluster the two crescents. 


First, we begin by creating a similariy matrixx of shape (n,n) that contains the distances between each pair of coordinates in the matrix. The library `sklearn` has a function we can use!


```python
from sklearn.metrics.pairwise import pairwise_distances
#Computes all the pairwise distances and collects them into a matrix
A=pairwise_distances(X)
A
```


    array([[0.        , 1.27292462, 1.33315598, ..., 1.9812102 , 1.68337039,
            1.94073324],
           [1.27292462, 0.        , 1.46325112, ..., 1.93729167, 1.68543003,
            1.91287315],
           [1.33315598, 1.46325112, 0.        , ..., 0.64857172, 0.35035968,
            0.60860868],
           ...,
           [1.9812102 , 1.93729167, 0.64857172, ..., 0.        , 0.30070415,
            0.04219636],
           [1.68337039, 1.68543003, 0.35035968, ..., 0.30070415, 0.        ,
            0.26255757],
           [1.94073324, 1.91287315, 0.60860868, ..., 0.04219636, 0.26255757,
            0.        ]])



Now, we want to code these distances into two categories: 1) if the distance is within a specified epsilon, the entry will be 1. If it's not within this distance, the entry will be 0. We need to make sure that the diagonal should be all zeroes because the distance between an entry and itself is 0.

In this part, we use epsilon as 0.4.


```python
#Uses boolean indexing to replace values of the matrixx
A[A>0.4]=0
A[A!=0]=1
#fills the diagonal with zeros
np.fill_diagonal(A,0)
```


```python
#checks that our matrix is symmetric
np.all(A.T==A)
```

    True

The matrix `A` now contains information about which points are near (within distance `epsilon`) which other points. In order to cluster the data points in `X`, then, we need to partition the rows and columns of `A`. 

To do this, we'll computer the binary norm cut objective for the matrix. However, for that, we first need to calculate the cut term and the volume for the a cluster. We'll find that a pair of clusters $$ C_0 $$ and $$ C_1 $$ is considered to be a "good" partition of the data when their norm-cut objective is small. 

First, we compute the cut term, which is the number of nonzero entries in a matrix that relates points in cluster $$C_0$$ to points in cluster $$C_1$$. We don't want the the points in $$C_0$$ to be very close to points in $$C_1$$. 

We write a function `cut(A,y)` to compute the cut term by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 


```python
    N = np.arange(200)
    #this array contains the elements in cluster 0
    c0 = N[y==0]
    #this array contains the elements in cluster 1
    c1 = N[y==1]
    #This is where the sum of all entries will be stored
    sum = 0
    #A nested for loop that iterates through each pair of i,j in matrix A
    for i in c0:
        for j in c1:
            #Adds the value of the i,j entry to the sum of all entries 
            sum += A[i,j]
    return sum
```


```python
cut1 = cut(A,y)
```


```python
cut1
```




    13.0

For our true clusters `y`, we get the cut objective as $$13$$. Now, we generate a random vector of random labels, and test our cut function on that.



```python
v = np.random.randint(0, 2, size = n)
v
cut2= cut(A,v)
cut2
```




    1116.0


We find that the cut term for the true clusters is much lower! This shows that this part of the cut objective indeed favors the true clusters over the random ones. 

Nowm we calculate the volume term. The volume of cluster $$C_0$$ is a measure of how "big" cluster $$C_0$$ is. 

To put it simply, according to the binary normcut objective term, we want to find clusters $$C_0$$ and $$C_1$$ such that:

1. There are relatively few entries of $$\mathbf{A}$$ that join $$C_0$$ and $$C_1$$. 
2. Neither $$C_0$$ and $$C_1$$ are too small. 

Now, we create a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$. We calculate this term according to the mathematical formula for the volume term. Then, we finally create a function to compute the norm cut objective using our cut and volume terms with the help of another mathematical formula.

```python
def vols(A,y):
    #Finds the ith row sum of A
    d = np.sum(A,axis=0)
    #Calculates this sum for cluster C0
    v0 = d[y==0].sum()
    #Calculates this sum for cluster C1
    v1 = d[y==1].sum()
    return v0,v1
def normcut(A,y):
    #Gets the volume term for C0 and C1
    v0,v1 = vols(A,y)
    #Cut term
    cut_var = cut(A,y)
    return cut_var*((1/v0)+(1/v1))
```


```python
norm = normcut(A,y)
norm
```




    0.011518412331615225


Yay! We get a very low value for the binary norm cut objective, as intended. Now, let's test our function on random values.

```python
norm_fake= normcut(A,v)
norm_fake
```




    0.995836336735081

As expected, the function does better on true clusters.

We have now defined a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $$A$$ and (b) not too small. We could try to find a cluster vector `y` such that `normcut(A,y)` is small, but that's a problem which is not possible to solve in practical time. So, we'll be using a linear algebra trick!

We define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ which uses this trick. The signs of  the elements of $$\mathbf{z}$$ contain all the information from $$\mathbf{y}$$. Basically, if $$i$$ is in cluster $$C_0$$, then $$y_i = 0$$ and $$z_i > 0$$. 

We now write a function called `transform(A,y)` to compute the $$\mathbf{z}$$ vector given `A` and `y`, using the math formula that we know. 

Then, we use some formulae to check that we're on the right path. 

```python
def transform(A,y):
    #Gets the volume term for C0
    v0 = vols(A,y)[0]
    #Gets the volume term for C1
    v1= vols(A,y)[1]
    #Creates array of length n
    z=np.zeros(n)
    #Sets the value of z[i] to 1/v0 if y[i]==0
    z[y==0]=1/v0
    #Sets the value of z[i] to -1/v1 if y[i]==1
    z[y==1]=-1/v1
    return z
z = transform(A,y)
D = np.zeros((n,n))
d = np.sum(A,axis=0)
np.fill_diagonal(D, d)
```

We now compare this matrix product's value to the value we got for the binary norm cut objective to double check our work, and it's the same :)


```python
np.isclose(z.T@((D-A)@z)/(z@D@z),norm)
```




    True



We also check if $$\mathbf{z}$$ should contain roughly as many positive as negative entries using this formula: 


```python
z@D@np.ones(n)
```




    0.0

And our result of zero implies that it does!

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing a particular function. We can use optimization to do this task, but we first need a function that substitutes for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. 


```python
def orth(u, v):
    return (u @ v) / (v @ v)*v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```
Now, we use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. I chose to use the Nelder-Mead method because I was getting an unsuccessful optimization if I didn't use a method. (Spoiler: I think this also helped the algorithm produce very accurate results later on, so that's great too).

```python
from scipy.optimize import minimize
#Minimizes the function orth_obj
z_ = minimize(orth_obj,z,method = 'Nelder-Mead')
```

We have made `z_min` in a way such that that only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. So, we can plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 


```python
#array containing the minimum values
z_min=z_["x"]
#Encodes the z_min matrix to have 0 or 1 according to its sign 
z_min[z_min>=0]=1
z_min[z_min<0]=0
z_min
```




    array([0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,
           1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,
           0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,
           1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,
           1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,
           0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0.,
           0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,
           0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0.,
           0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,
           0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1.,
           0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1.,
           0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1.])




```python
plt.scatter(X[:,0], X[:,1],c=z_min)
```




    <matplotlib.collections.PathCollection at 0x1210e6370>


![output_35_1.png](/images/output_35_1.png)

    
Woah. It looks like the clustering is pretty accurate!

This was cool. However, actually calculating and optimizing the orthogonal objective is too slow to be practical. 

What's special about spectral clustering is that it uses eigenvalues and eigenvectors of matrices to do the clustering. 

According to the math done by other great people, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

So, now, we create what is called the *Laplacian* matrix of the similarity matrix $$\mathbf{A}$$. 
We then find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`. 

```python
D_inv = np.linalg.inv(D) #Inverse of the D matrix
L = D_inv@(D-A) #Creates the matrix L
Lam, U = np.linalg.eig(L) #Finds the eigenvalues and their corresponding eigenvectors
```


```python
z_eig=U[:,1] #Finds the eigenvector corresponding to the second smallest eigenvalue
```


```python
#Encodes the z_eig matrix to have 0 or 1 according to its sign 
z_eig = z_eig<0
```




    array([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,
           0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
           1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
           0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
           0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,
           1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
           1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
           1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
           1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
           1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
           1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
           1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.])


Now, we plot the data again, using the sign of `z_eig` as the color. 

```python
plt.scatter(X[:,0], X[:,1],c=z_eig)
```




    <matplotlib.collections.PathCollection at 0x1212c4130>



    
![output_40_1.png](/images/output_40_1.png)


This looks pretty accurate too! 

Now, we synthesize everything we've done till now into a (relatively) small function: `spectral_clustering(X, epsilon)` .

```python
def spectral_clustering(X, epsilon):
    '''
    Takes it data and a distance threshold and performs spectral 
    clustering to return an array of binary labels indicating whether 
    data point i is in group 0 or group 1.  
    
    Parameters: 
        X (array): The data to be clustered
        epsilon (int or float): An int/float specifying the distance threshold
        
    Returns: 
        z_eig (array): An array of binary labels indicating whether 
                       data point i is in group 0 or group 1
    '''
    #Computes all the pairwise distances and collects them into a matrix
    A=pairwise_distances(X)
    #Replaces the values according to the distance threshold
    A[A>epsilon]=0
    A[A!=0]=1
    #Fills the diagonal of the matrix with 0
    np.fill_diagonal(A,0)
    #Creates a diagonal matrix containing the sum of the ith row of A
    D = np.diag(A.sum(axis = 1))
    #Creates the matrix L
    L = np.linalg.inv(D)@(D-A)
    #Finds the eigenvalues and their corresponding eigenvectors
    Lam, U = np.linalg.eig(L)
    #Finds the eigenvector corresponding to the second smallest eigenvalue
    z_eig=U[:,1]
    #Encodes the z_eig matrix to have 0 or 1 according to its sign 
    return z_eig < 0
```

Phew, we're done with the setting up of our algorithm. Now, we experiment with our function. First, we generate a different data seed and increase n to 1000.

```python
np.random.seed(2000)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x12270f400>


![output_45_1.png](/images/output_45_1.png)


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.7))
```




    <matplotlib.collections.PathCollection at 0x120bdfdf0>

![output_46_2.png](/images/output_46_2.png)


Our algorithm does a pretty good job. Let us see what happens if we increase the noise.

```python
np.random.seed(2000)
n = 1000
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.12, random_state=None)
plt.scatter(X[:,0], X[:,1])
```

    <matplotlib.collections.PathCollection at 0x11d159fd0>



![output_noise.png](/images/output_noise.png)

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.4))
```


    <matplotlib.collections.PathCollection at 0x11da74460>

    
![output_noise_1.png](/images/output_noise_1.png)
    
Our algorithm doesn't work too well on increased noise with the current settings, so I reduce our epsilon parameter (randomly) to see if that helps.


```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.2))
```

    <matplotlib.collections.PathCollection at 0x126262a60> 

    
![output_noise_2.png](/images/output_noise_2.png)

That looks much better!
    
Now we try our spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x120cab7f0>


![output_48_1.png](/images/output_48_1.png)




There are two concentric circles. As we saw before, k-means will not do well here at all. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x12649adf0>


![output_50_1.png](/images/output_50_1.png)

        
So, here comes in our spectral clustering algorithm.



```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, 0.5))
```




    <matplotlib.collections.PathCollection at 0x1236c4790>

![output_52_1.png](/images/output_52_1.png)

    
The algorithm gets this perfectly!

<div class="got-help">
A few of the changes that I made based on my peers' feedback include:
1. Vectorizing the way I calculated the cluster matrices within the cut(A,y) function
2. Using np.isclose() to make the similarity more explicit (originally, I just put the numbers more side to side).
3. Explaining why I used a method for my optimization function.
4. Some other minor but super helpful changes like typos. 
Originally, I generated the numbers like this: 
</div>

<div class="gave-help">
Something that I'm happy with on this blog is my use of vectorization methods over for loops, and this is something that I helped one of my peers with as well. I suggested to them a way of writing the `vols(A,y)` function in a way that does not use for loops but instead relies on boolean indexing and np.sum(). </div>
{::options parse_block_html="false" /}



